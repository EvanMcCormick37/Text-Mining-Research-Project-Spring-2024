{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240b5735",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "To compile a list of news stories and articles about the **transgender** topic from the past year, I used three different sources:\n",
    "\n",
    "    1. NewsAPI.org - A free but limited API for compiling News Articles related to one topic. I scraped a reasonable amount of stories from here.\n",
    "    2. WorldNewsAPI.com - Another free news API with a global scope of news articles and a greater free query limit. It also provided some interesting metadata for each article, such as a sentiment analysis run by the WorldNews organization.\n",
    "    3. GroundNews.com - A paid news service which provides a wealth of interesting metadata and analysis on the News Stories covered there. I have a paid subscription to this service, and was able to scrape a decent number of news articles, along with pertinent metadata.\n",
    "\n",
    "Two of these sources were APIs, which gave me the text in a relatively easy to clean format. The third, GroundNews, gave me a bit of a challenge. Scraping the article summaries and metadata actually proved relatively simple, but I then tried to scrape the source texts (from the original stories) which proved tricky, and I had to get creative to glean usable text data from those sources.\n",
    "\n",
    "Let's start with the APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8e6b4",
   "metadata": {},
   "source": [
    "# News API\n",
    "\n",
    "For this one the strategy was relatively simple. I first acquired an API Key from the website, then ran the following series of queries to acquire the maximum number of articles possible. The real difficulty was in overcoming the restrictions this API placed upon free users. I was limited in both number of requests, and the page # which I could use for a request result. In this case, I was limited to 5 pages per request, and to get around this limitation I retried the query multiple times using each available sorting parameter. I then combined these results into a single dataframe containing all metadata and dropped duplicates.\n",
    "\n",
    "Here's what I did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e96dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The owner of a restaurant in New York state to...</td>\n",
       "      <td>Some workers at the pizzeria even equated bein...</td>\n",
       "      <td>Lourdes Balduque/Getty Images\\r\\n&lt;ul&gt;&lt;li&gt;Staff...</td>\n",
       "      <td>Grace Dean</td>\n",
       "      <td>2024-01-29T14:33:43Z</td>\n",
       "      <td>business-insider</td>\n",
       "      <td>https://www.businessinsider.com/new-york-resta...</td>\n",
       "      <td>https://i.insider.com/65b796ba6c8f0a134f7aa55e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man guilty of killing transgender woman in hat...</td>\n",
       "      <td>It was the United States' first federal trial ...</td>\n",
       "      <td>In combo of undated selfie images provided cou...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-02-24T05:40:52Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.npr.org/2024/02/24/1233685859/man-...</td>\n",
       "      <td>https://media.npr.org/assets/img/2024/02/24/ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Utah Joins 10 Other States in Regulating Bathr...</td>\n",
       "      <td>Utah became the latest state to regulate bathr...</td>\n",
       "      <td>Utah became the latest state to regulate bathr...</td>\n",
       "      <td>AMY BETH HANSON / AP</td>\n",
       "      <td>2024-01-31T16:25:37Z</td>\n",
       "      <td>time</td>\n",
       "      <td>https://time.com/6590528/utah-joins-states-reg...</td>\n",
       "      <td>https://api.time.com/wp-content/uploads/2024/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Killer was moved to Brianna Ghey's school afte...</td>\n",
       "      <td>Scarlett Jenkinson was moved after drugging a ...</td>\n",
       "      <td>Killer Scarlett Jenkinson was moved to a new s...</td>\n",
       "      <td>https://www.facebook.com/bbcnews</td>\n",
       "      <td>2024-02-02T12:30:54Z</td>\n",
       "      <td>bbc-news</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-68153179</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South Dakota has apologized and must pay $300K...</td>\n",
       "      <td>None</td>\n",
       "      <td>Si vous cliquez sur « Tout accepter », nos par...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-02-06T18:31:04Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://consent.yahoo.com/v2/collectConsent?se...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The owner of a restaurant in New York state to...   \n",
       "1  Man guilty of killing transgender woman in hat...   \n",
       "2  Utah Joins 10 Other States in Regulating Bathr...   \n",
       "3  Killer was moved to Brianna Ghey's school afte...   \n",
       "4  South Dakota has apologized and must pay $300K...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Some workers at the pizzeria even equated bein...   \n",
       "1  It was the United States' first federal trial ...   \n",
       "2  Utah became the latest state to regulate bathr...   \n",
       "3  Scarlett Jenkinson was moved after drugging a ...   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \\\n",
       "0  Lourdes Balduque/Getty Images\\r\\n<ul><li>Staff...   \n",
       "1  In combo of undated selfie images provided cou...   \n",
       "2  Utah became the latest state to regulate bathr...   \n",
       "3  Killer Scarlett Jenkinson was moved to a new s...   \n",
       "4  Si vous cliquez sur « Tout accepter », nos par...   \n",
       "\n",
       "                             author           publishedAt            source  \\\n",
       "0                        Grace Dean  2024-01-29T14:33:43Z  business-insider   \n",
       "1                              None  2024-02-24T05:40:52Z              None   \n",
       "2              AMY BETH HANSON / AP  2024-01-31T16:25:37Z              time   \n",
       "3  https://www.facebook.com/bbcnews  2024-02-02T12:30:54Z          bbc-news   \n",
       "4                              None  2024-02-06T18:31:04Z              None   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.businessinsider.com/new-york-resta...   \n",
       "1  https://www.npr.org/2024/02/24/1233685859/man-...   \n",
       "2  https://time.com/6590528/utah-joins-states-reg...   \n",
       "3             https://www.bbc.co.uk/news/uk-68153179   \n",
       "4  https://consent.yahoo.com/v2/collectConsent?se...   \n",
       "\n",
       "                                          urlToImage  \n",
       "0  https://i.insider.com/65b796ba6c8f0a134f7aa55e...  \n",
       "1  https://media.npr.org/assets/img/2024/02/24/ap...  \n",
       "2  https://api.time.com/wp-content/uploads/2024/0...  \n",
       "3  https://ichef.bbci.co.uk/news/1024/branded_new...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame(columns=['title','description','content','author','publishedAt','source','url','urlToImage'])\n",
    "print('running!')\n",
    "\n",
    "#Changing sorting parameters to get around the free API limitations.\n",
    "#There is a maximum page limit of 5, limiting the number of results seen even further than would be possible given the 100 queries/day limitation.\n",
    "\n",
    "#Articles sorted by publish date by default\n",
    "for num in range(1,6):  \n",
    "    request_params = {'q':'transgender',\n",
    "                      'page':f'{num}',\n",
    "                      'language':'en',\n",
    "                      'apiKey':'edb887b5553c43aab598452a6335ad0c'}\n",
    "    response = requests.get('https://newsapi.org/v2/everything',request_params)\n",
    "    \n",
    "    json = response.json()\n",
    "    articles = json['articles']\n",
    "        \n",
    "    for article in articles:\n",
    "        df1.loc[len(df1.index)]=[\n",
    "        article['title'],\n",
    "        article['description'],\n",
    "        article['content'],\n",
    "        article['author'],\n",
    "        article['publishedAt'],\n",
    "        article['source']['id'],\n",
    "        article['url'],\n",
    "        article['urlToImage']\n",
    "        ]\n",
    "        \n",
    "df2 = pd.DataFrame(columns=['title','description','content','author','publishedAt','source','url','urlToImage'])\n",
    "\n",
    "#Same query sorted by relevancy\n",
    "for num in range(1,6):  \n",
    "    request_params = {'q':'transgender',\n",
    "                      'page':f'{num}',\n",
    "                      'sortBy':'relevancy',\n",
    "                      'language':'en',\n",
    "                      'apiKey':'edb887b5553c43aab598452a6335ad0c'}\n",
    "    response = requests.get('https://newsapi.org/v2/everything',request_params)\n",
    "    \n",
    "    json = response.json()\n",
    "    articles = json['articles']\n",
    "        \n",
    "    for article in articles:\n",
    "        df2.loc[len(df2.index)]=[\n",
    "        article['title'],\n",
    "        article['description'],\n",
    "        article['content'],\n",
    "        article['author'],\n",
    "        article['publishedAt'],\n",
    "        article['source']['id'],\n",
    "        article['url'],\n",
    "        article['urlToImage']\n",
    "        ]\n",
    "        \n",
    "df3 = pd.DataFrame(columns=['title','description','content','author','publishedAt','source','url','urlToImage'])\n",
    "\n",
    "#Same Query sorted by popularity\n",
    "for num in range(1,6):  \n",
    "    request_params = {'q':'transgender',\n",
    "                      'page':f'{num}',\n",
    "                      'sortBy':'popularity',\n",
    "                      'language':'en',\n",
    "                      'apiKey':'edb887b5553c43aab598452a6335ad0c'}\n",
    "    response = requests.get('https://newsapi.org/v2/everything',request_params)\n",
    "    \n",
    "    json = response.json()\n",
    "    articles = json['articles']\n",
    "        \n",
    "    #I kept every piece of information attached to each article in its own column in the raw text dataframe, even if the last two ultimately proove irrelevant. \n",
    "    #Better to have extra irrelevant data than leave out something important.\n",
    "    for article in articles:\n",
    "        df3.loc[len(df3.index)]=[\n",
    "        article['title'],\n",
    "        article['description'],\n",
    "        article['content'],\n",
    "        article['author'],\n",
    "        article['publishedAt'],\n",
    "        article['source']['id'],\n",
    "        article['url'],\n",
    "        article['urlToImage']\n",
    "        ]\n",
    "\n",
    "df_all = pd.concat([df1,df2,df3]).drop_duplicates()\n",
    "    \n",
    "# df_all.to_csv('../../data/newsapi_corpus_raw.csv')\n",
    "\n",
    "df_all.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06ca75",
   "metadata": {},
   "source": [
    "# WorldNewsAPI\n",
    "\n",
    "This API was more friendly to the free user than NewsAPI, with much less strict limitations. Here I was limited to a maximum of 50 *points* per day, with each API request costing at least one point, plus an additional 1 point for every 100 results returned. However, this meant that with careful typing and some luck, I could pull over 4,000 articles in one go!\n",
    "\n",
    "Here was my final code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['title','text','authors','country','sentiment','url'])\n",
    "\n",
    "#Going to extract a bunch of news articles from the past couple of months from this API as well.\n",
    "url = 'https://api.worldnewsapi.com/search-news'\n",
    "\n",
    "#There is a maximum of 100 returned articles per request. Make 30 such requests and append the results of each into a single data-frame.\n",
    "# o is the request offset. It decides which batch of 100 we're requesting.\n",
    "for o in range(0,3001,100):\n",
    "    request_params = {'text':'transgender',\n",
    "                      'number':'100',\n",
    "                      'offset':f'{o}',\n",
    "                      'earliest-publish-date':'2023-09-01',\n",
    "                      'sort':'publish-time',\n",
    "                      'language':'en',\n",
    "                      'api-key':'968a873ef4a14e2fb2acc6a0107ccbea'}\n",
    "    print(f'requesting articles {o+1}-{o+101}!')\n",
    "    #get the news list from the response json.\n",
    "    resp = requests.get(url,request_params)\n",
    "    json = resp.json()\n",
    "    articles =json['news']\n",
    "    #Store the relevant metadata for each article in the corresponding column and append that row to my dataframe.\n",
    "    for article in articles:\n",
    "        df.loc[len(df.index)]=[\n",
    "        article['title'],\n",
    "        article['text'],\n",
    "        #Authors is often a list of people, but is sometimes empty. This ensures that this column contains all strings.\n",
    "        article['authors'][0] if len(article['authors'])>0 else '',\n",
    "        article['source_country'],\n",
    "        article['sentiment'],\n",
    "        article['url']\n",
    "        ]\n",
    "        \n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('../../data/worldnewsapi_corpus_raw.csv',mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
